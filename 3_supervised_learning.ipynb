{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "&rarr; find correlations between variables; predicting/estimating output variables based on one or more input variables\n",
    "\n",
    "- Given an input ($x$, samples of one or more variables) and an output response ($y$, samples of one or more other variables of the dataset), we have to find the relationship ($f$) between them: $y = f(df_X) + e$\n",
    "    - where \"$e$\" is called **bias**, a random error that is independent of input and has mean zero\n",
    "- The predicted/estimated function ($\\hat{f}$) generates, with the same input ($df_X$), a resulting output ($\\hat{y}$, the estimated samples): $\\hat{y}=\\hat{f}(df_X)$\n",
    "- The accuracy lies on the similarity between $y$ and $\\hat{y}$ (between the real samples and the predicted ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&rarr; **Regression**: predicting continuous (quantitative) value <br>\n",
    "&rarr; **Classification**: predicting a discrete value, that corresponds to whether your sample belongs to a class or not\n",
    "\n",
    "### Metrics:\n",
    "- https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "- Classification model's Accuracy:\n",
    "    - Mean between the error rate of all the predictions; error rate is 1 if wrong prediction, 0 otherwise\n",
    "- Regression model's Accuracy:\n",
    "    - **Residual Sum of Squares (RSS)**: the sum of the squares of all residuals, which are the distance between the predicted value and the real one. A problem is the value that increases with the number of samples. $$ RSS = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 $$\n",
    "    - **Mean Squared Error (MSE)**: the mean of all the distances (squared) between a predicted sample and the real one $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 $$\n",
    "    - **Mean Absolute Error (MAE)**: $$ MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y_i}| $$\n",
    "    - **Residual Standard Error (RSE)**: absolute measure of lack of fit of the model. RSS normalized with respect to n and p: $$ RSE = \\sqrt{\\frac{1}{n - p - 2} \\times RSS} $$\n",
    "    - **Explained Sum of Squares (ESS)**: quantity of variance explained by the model. How much the predicted data differ from the mean: $$ ESS = \\sum_{i=1}^n (\\hat{y_i} - \\overline{y})^2 $$\n",
    "\n",
    "    - **Total Sum of Squares (TSS)**: Total variance in the response data. How much the real data differ from the mean: $$ TSS = ESS + RSS = \\sum_{i=1}^n (y_i - \\overline{y})^2 $$\n",
    "    - **R^2**\n",
    "        $$ R^2 = \\frac{ESS}{TSS} = 1 - \\frac{RSS}{TSS} $$\n",
    "        - close to 1: the model fits well the data\n",
    "        - close to 0: linear model is wrong, or bias error is high\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Over/Underfitting\n",
    "\n",
    "- The estimation of $f$ can be made by a parametric approach: it is generally much easier to estimate a set of parameters, than it is to fit an entirely arbitrary function.\n",
    "    -  **Overfitting**: the model we choose will usually not match the true unknown form of $f$. If the model is too flexible (too similar to $f$), the training error will be too low and the test error can be high. The solution is to change/remove parameters or simplify the model.\n",
    "    - **Underfitting** happens when you underestimate the model.\n",
    "- It is important to decide for any given set of data which method produces the best results\n",
    "    - **Linear** models are simple but often too inaccurate\n",
    "    - **Highly non-linear** models can potentially provide more accurate predictions, but far more complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "random_state = 42\n",
    "\n",
    "df = pd.read_csv(\"datasets/nba.csv\")\n",
    "# Original dataset (except the target variable we want to predict)\n",
    "df_X = df.dropna().loc[:, df.columns != \"AST\"].select_dtypes(include=\"number\")\n",
    "# Original variable we want to predict\n",
    "df_y = df.dropna()[\"AST\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation/Test split\n",
    "\n",
    "- The Training Set is the set of samples used for training our model\n",
    "- The Validation Set is the set of samples we used to tune the hyperparameters of our model. ***Calibrating the hyperparameters on the test set means overestimating the performance***\n",
    "- The Test Set is the set of samples on which to evaluate the final performance of the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\t (7836, 22)\n",
      "X_train: (3937, 22) , % 50.242470648289945\n",
      "X_val:\t (1313, 22) , % 16.755997958141908\n",
      "X_test:\t (2586, 22) , % 33.00153139356815\n",
      "y:\t (7836,)\n",
      "y_train: (3937,)\n",
      "y_val:\t (1313,)\n",
      "y_test:\t (2586,)\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#########################################################################\n",
    "# CLASSIC SPLIT\n",
    "#########################################################################\n",
    "\n",
    "# Split X and y into train and test, with test size of 33% of the total\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_X, df_y, test_size=0.33, random_state=random_state\n",
    ")\n",
    "# Split X_train and y_train into train and valuation, with valuation size of 25% of the train total\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=random_state\n",
    ")\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "# PERSONALIZED train_validation_test_split()\n",
    "#########################################################################\n",
    "\n",
    "\n",
    "def train_validation_test_split(\n",
    "    *arrays, train_size=None, validation_size=None, test_size=None, random_state=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits a dataset into train, validation, and test sets based on the provided proportions.\n",
    "\n",
    "    # Parameters:\n",
    "        *arrays : sequence of indexables with same length / shape[0]\n",
    "            Allowed inputs are lists, numpy arrays, scipy-sparse\n",
    "            matrices or pandas dataframes.\n",
    "        train_size (float):\n",
    "            Proportion of the dataset for the training set (0.0 to 1.0).\n",
    "        validation_size (float):\n",
    "            Proportion of the dataset for the validation set (0.0 to 1.0).\n",
    "        test_size (float):\n",
    "            Proportion of the dataset for the test set (0.0 to 1.0).\n",
    "\n",
    "    # Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test: Split datasets.\n",
    "    \"\"\"\n",
    "    if train_size and validation_size:\n",
    "        test_size = 1.0 - train_size - validation_size\n",
    "    elif test_size and validation_size:\n",
    "        train_size = 1.0 - test_size - validation_size\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Provide either train and validation sizes or test and validation sizes.\"\n",
    "        )\n",
    "\n",
    "    if test_size < 0 or validation_size < 0 or train_size < 0:\n",
    "        raise ValueError(\"The total sum must be a value < 1.0\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        *arrays, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        test_size=validation_size\n",
    "        * (X_train.shape[0] + X_test.shape[0])\n",
    "        / X_train.shape[0],\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_validation_test_split(\n",
    "    df_X, df_y, test_size=0.33, validation_size=0.1675\n",
    ")\n",
    "\n",
    "print(\"X:\\t\", df_X.shape)\n",
    "print(\"X_train:\", X_train.shape, \", %\", X_train.shape[0] * 100 / df_X.shape[0])\n",
    "print(\"X_val:\\t\", X_val.shape, \", %\", X_val.shape[0] * 100 / df_X.shape[0])\n",
    "print(\"X_test:\\t\", X_test.shape, \", %\", X_test.shape[0] * 100 / df_X.shape[0])\n",
    "print(\"y:\\t\", df_y.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_val:\\t\", y_val.shape)\n",
    "print(\"y_test:\\t\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Cross Validation with *Train-Validation-Test Split*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on VALIDATION SET: 0.75000 K = 1\n",
      "Accuracy on VALIDATION SET: 0.75000 K = 6\n",
      "Accuracy on VALIDATION SET: 0.72222 K = 11\n",
      "Accuracy on VALIDATION SET: 0.69444 K = 16\n",
      "Accuracy on VALIDATION SET: 0.72222 K = 21\n",
      "\n",
      "Best validation model, K = 1\n",
      "Accuracy on TEST SET: 0.72222\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "wine = datasets.load_wine()\n",
    "X = wine[\"data\"]\n",
    "y = wine[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=random_state\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=random_state\n",
    ")\n",
    "\n",
    "\n",
    "clf = dict()\n",
    "best_acc = 0\n",
    "best_acc_ind = -1\n",
    "\n",
    "for i in range(1, 22, 5):\n",
    "    clf[i] = KNeighborsClassifier(n_neighbors=i)\n",
    "\n",
    "    # Fit on TRAIN SET\n",
    "    clf[i].fit(X_train, y_train)\n",
    "\n",
    "    # Score on VALIDATION SET\n",
    "    acc = clf[i].score(X_val, y_val)\n",
    "\n",
    "    if best_acc < acc:\n",
    "        best_acc = acc\n",
    "        best_acc_ind = i\n",
    "    print(\"Accuracy on VALIDATION SET: {:.5f}\".format(acc), \"K =\", i)\n",
    "\n",
    "print(\"\\nBest validation model, K =\", best_acc_ind)\n",
    "\n",
    "# Test on TEST SET\n",
    "acc = clf[best_acc_ind].score(X_test, y_test)\n",
    "print(\"Accuracy on TEST SET: {:.5f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Cross Validation with k-Fold\n",
    "- Often there is not a designated test set to calculate the model validity through the test error.\n",
    "- **Leave-One-Out Cross-Validation**. To estimate the test(/validation) error, leave out one sample, fit the model on the remaining samples, and finally test the model on the left-out sample. Then repeat for every sample, and calculate the final error as the mean of all errors. It's very very expensive because there are N iterations.\n",
    "- **k-Fold Cross Validation**. Randomly divide the set of samples into k groups, or folds, of approximately equal size. The first fold is left out, the remaining are fitted, and finally the model is tested on the fold left out. Repeat for every fold. Typically 3 to 5 folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Preprocessing and *Data Leakage*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "random_state = 42\n",
    "dataset = datasets.load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset[\"data\"], dataset[\"target\"], test_size=0.4, random_state=random_state\n",
    ")\n",
    "\n",
    "# PREPROCESSING\n",
    "# WARNING!!!: preprocessing the data with Min-Max or StandardScaling (in general with all normalizations that follow the vertical axes) and then performing a cross_val_score() introduces *DATA LEAKAGE*. This is because cross_val_score() splits the train set in \"cv\" folds at each iteration, and uses only \"cv-1\" for training. The left-out is used for validation, that is to calculate the error. But this left-out fold is already preprocessed with the scaler taken from all the \"cv\" folds, and not only on the \"cv-1\" train folds. Therefore this left-out validation fold is preprocessed even with his own data, *and this means overstimate the performance*.\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_p = scaler.transform(X_train)\n",
    "X_test_p = scaler.transform(X_test)\n",
    "\n",
    "# Parameters to keep the best results\n",
    "best_acc = 0\n",
    "bestK = \"linear\"\n",
    "\n",
    "# KFold\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "\n",
    "# Iterate on an hyperparameter\n",
    "for kernel in [\"linear\", \"poly\", \"rbf\"]:\n",
    "    clf = SVC(kernel=kernel, random_state=random_state)\n",
    "    # Cross Validate\n",
    "    scores = cross_val_score(clf, X_train_p, y_train, cv=cv, n_jobs=-1)\n",
    "\n",
    "    # save best result so far\n",
    "    acc = np.mean(scores)\n",
    "    print(\n",
    "        \"Cross validation score: {:.3f}\".format(acc),\n",
    "        \"kernel =\",\n",
    "        kernel,\n",
    "    )\n",
    "    if best_acc < acc:\n",
    "        best_acc = acc\n",
    "        bestK = kernel\n",
    "\n",
    "\n",
    "print(\"Best Kernel:\", bestK)\n",
    "clf = SVC(kernel=bestK, random_state=random_state)\n",
    "clf.fit(X_train_p, y_train)\n",
    "print(\"Test set accuracy: {:.5f}\".format(clf.score(X_test_p, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Cross Validation with GridSearch\n",
    "- Find the best hyperparameters, trying every combination of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Preprocessing and no *Data Leakage*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# a function with different normalization and scaling techniques\n",
    "def preprocess(X_train, X_test, modality):\n",
    "    X_train_p = X_train\n",
    "    X_test_p = X_test\n",
    "\n",
    "    if modality == \"l2\" or modality == \"l1\":\n",
    "        X_train_p = preprocessing.normalize(X_train, norm=modality)\n",
    "        X_test_p = preprocessing.normalize(X_test, norm=modality)\n",
    "\n",
    "    if modality == \"standard\":\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_p = scaler.transform(X_train)\n",
    "        X_test_p = scaler.transform(X_test)\n",
    "\n",
    "    if modality == \"min-max\":\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_p = scaler.transform(X_train)\n",
    "        X_test_p = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_p, X_test_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: svc , best params: {'svc__C': 0.1, 'svc__kernel': 'linear'} , score: 0.9437908496732026\n",
      "Steps: std-scaler , best params: {'svc__C': 0.1, 'svc__kernel': 'linear'} , score: 0.9777777777777779\n",
      "Steps: min-max , best params: {'svc__C': 1, 'svc__kernel': 'linear'} , score: 0.9888888888888889\n",
      "\n",
      "Parameter used for test set: {'svc__C': 1, 'svc__kernel': 'linear'} , scaling: min-max\n",
      "Test set accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import datasets, preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "dataset = datasets.load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset[\"data\"], dataset[\"target\"], test_size=0.5, random_state=random_state\n",
    ")\n",
    "\n",
    "# Grid of hyperparameters\n",
    "param_grid = {\n",
    "    \"svc__kernel\": [\"linear\", \"poly\", \"rbf\"],\n",
    "    \"svc__C\": [0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "# Parameters to keep the best results\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "bestP = \"no\"\n",
    "\n",
    "# Iterate over Preprocess types\n",
    "prep = [\n",
    "    Pipeline([(\"svc\", SVC())]),\n",
    "    Pipeline([(\"std-scaler\", preprocessing.StandardScaler()), (\"svc\", SVC())]),\n",
    "    Pipeline([(\"min-max\", preprocessing.MinMaxScaler()), (\"svc\", SVC())]),\n",
    "]\n",
    "\n",
    "for pipe in prep:\n",
    "    search = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=5)\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    print(\n",
    "        \"Steps:\",\n",
    "        pipe.steps[0][0],\n",
    "        \", best params:\",\n",
    "        search.best_params_,\n",
    "        \", score:\",\n",
    "        search.best_score_,\n",
    "    )\n",
    "    if search.best_score_ > best_score:\n",
    "        best_score = search.best_score_\n",
    "        best_params = search.best_params_\n",
    "        bestP = pipe.steps[0][0]\n",
    "\n",
    "# test with best configuration\n",
    "clf = SVC(\n",
    "    kernel=best_params[\"svc__kernel\"],\n",
    "    C=best_params[\"svc__C\"],\n",
    "    random_state=random_state,\n",
    ")\n",
    "X_train_p, X_test_p = preprocess(X_train, X_test, bestP)\n",
    "clf.fit(X_train_p, y_train)\n",
    "print(\"\\nParameter used for test set:\", best_params, \", scaling:\", bestP)\n",
    "print(\"Test set accuracy: {:.2f}\".format(clf.score(X_test_p, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
