{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.tensorboard import SummaryWriter  # log writer per visualizzare le loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility, fix all the seeds\n",
    "\n",
    "\n",
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Data Layer\n",
    "# It must be extended to implement three methods\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)  # the type is choosen by the type of the data\n",
    "        self.y = torch.LongTensor(y)\n",
    "\n",
    "        self.num_features = X.shape[1]\n",
    "        self.num_classes = len(np.unique(y))\n",
    "\n",
    "    def __len__(self):  # how many samples\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(\n",
    "        self, idx\n",
    "    ):  # return a single element of dataset (single sample or batch) (if the dataset is not in memory, it can read from file system and return the object)\n",
    "        return self.X[idx, :], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple neural network\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Two layers\n",
    "        self.fc2 = nn.Linear(self.hidden_size, num_classes)  # hidden to output\n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden_size)  # input to hidden\n",
    "\n",
    "        # activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    # How layers are connected between them. This even defines the graph of backpropagation.\n",
    "    def forward(self, x):\n",
    "        h = self.fc1(x)  # first layer\n",
    "        h = self.relu(h)  # activation function\n",
    "        output = self.fc2(h)  # second layer\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for the training process\n",
    "# model: instance of class to train\n",
    "# criterion: loss function (cross-entropy, ...)\n",
    "# optimizer\n",
    "# epoch: number of\n",
    "# train_loader and val_loader: who gives the batches\n",
    "# device: cpu/gpu where i train\n",
    "# writer and log_name: logging\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    writer,\n",
    "    log_name=\"model\",\n",
    "):\n",
    "    n_iter = 0\n",
    "    best_valid_loss = float(\"inf\")  # worst valid loss\n",
    "    for epoch in range(epoch):\n",
    "        model.train()  # activate train dropout level\n",
    "\n",
    "        for (\n",
    "            data,\n",
    "            targets,\n",
    "        ) in train_loader:  # get_item from MyDataset class (single item or batch)\n",
    "            data, targets = data.to(device), targets.to(device)  # move data and targets to cpu/gpu\n",
    "\n",
    "            optimizer.zero_grad()  # gradient to zero\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(data)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred, targets)\n",
    "            writer.add_scalar(\"Loss/train\", loss, n_iter)  # plot the batches\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_iter += 1\n",
    "\n",
    "        labels, _, y_pred = test_model(model, val_loader, device)\n",
    "        loss_val = criterion(y_pred, labels)\n",
    "        writer.add_scalar(\"Loss/val\", loss_val, epoch)  # plot the epochs\n",
    "\n",
    "        # save best model\n",
    "        if loss_val.item() < best_valid_loss:\n",
    "            best_valid_loss = loss_val.item()\n",
    "            if not os.path.exists(\"models\"):\n",
    "                os.makedirs(\"models\")\n",
    "            torch.save(model.state_dict(), \"models/\" + log_name)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to evaluate the performance on validation and test sets\n",
    "\n",
    "\n",
    "def test_model(model, data_loader, device):\n",
    "    model.eval()  # activate evaluation dropout level\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "\n",
    "    for data, targets in data_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        y_pred.append(model(data))  # accumulate predictions\n",
    "        y_test.append(targets)  # accumulate labels\n",
    "\n",
    "    y_test = torch.stack(y_test).squeeze()\n",
    "    y_pred = torch.stack(y_pred).squeeze()\n",
    "    y_pred_c = y_pred.argmax(\n",
    "        dim=1, keepdim=True\n",
    "    ).squeeze()  # return max position of prediction array, that is the class I will associate with the sample\n",
    "\n",
    "    return y_test, y_pred_c, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# look for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# Train hyperparameters\n",
    "num_epochs = 100  # try 100, 200, 500\n",
    "learning_rate = 0.01\n",
    "batch = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_iris()\n",
    "X = data[\"data\"]\n",
    "y = data[\"target\"]\n",
    "indices = np.arange(X.shape[0])  # trick of Pytorch to split the data in train/val/test\n",
    "\n",
    "# Separate indices\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, stratify=y, random_state=seed)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.2, stratify=y[train_idx], random_state=seed)\n",
    "\n",
    "\n",
    "# Scale data\n",
    "train_mean = np.mean(X[train_idx, :], axis=0)\n",
    "train_std = np.std(X[train_idx, :], axis=0)  # use only train\n",
    "X = (X - train_mean) / train_std  # but apply to all dataset\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "my_dataset = MyDataset(X, y)\n",
    "\n",
    "\n",
    "# Create subsets and relative dataloader\n",
    "train_subset = Subset(my_dataset, train_idx)\n",
    "train_loader = DataLoader(train_subset, batch_size=batch, shuffle=True)\n",
    "\n",
    "val_subset = Subset(my_dataset, val_idx)\n",
    "val_loader = DataLoader(val_subset, batch_size=1)\n",
    "\n",
    "test_subset = Subset(my_dataset, test_idx)\n",
    "test_loader = DataLoader(test_subset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Tensorboard from the command line:\n",
    "\n",
    "\"tensorboard --logdir runs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before training: 0.6666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after training: 0.8\n"
     ]
    }
   ],
   "source": [
    "# fix the seed for reproducibility\n",
    "fix_random(seed)\n",
    "\n",
    "\n",
    "# Start tensorboard\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "# Define the architecture, loss and optimizer\n",
    "hidden_size = 32  # architecture hyperparameter\n",
    "model = FeedForward(my_dataset.num_features, hidden_size, my_dataset.num_classes)\n",
    "model.to(device)  # move the NN to device\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  # W and beta and learning rate\n",
    "\n",
    "\n",
    "# Test before the training\n",
    "y_test, y_pred_c, _ = test_model(model, test_loader, device)\n",
    "acc = (y_test == y_pred_c).float().sum() / y_test.shape[0]\n",
    "print(\"Accuracy before training:\", acc.cpu().numpy())\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model = train_model(model, criterion, optimizer, num_epochs, train_loader, val_loader, device, writer)\n",
    "\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"models/model\"))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Test after the training\n",
    "y_test, y_pred_c, _ = test_model(model, test_loader, device)\n",
    "acc = (y_test == y_pred_c).float().sum() / y_test.shape[0]\n",
    "print(\"Accuracy after training:\", acc.cpu().numpy())\n",
    "\n",
    "\n",
    "# Close tensorboard writer after a training\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a new architecture\n",
    "\n",
    "\n",
    "class FeedForwardPlus(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, depth=1):\n",
    "        super(FeedForwardPlus, self).__init__()\n",
    "\n",
    "        model = [\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "        ]\n",
    "\n",
    "        # Set of pytorch modules\n",
    "        block = [\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),  # BatchNorm 1 dimention\n",
    "            nn.ReLU(),\n",
    "        ]\n",
    "\n",
    "        for i in range(depth):\n",
    "            model += block\n",
    "\n",
    "        # Create sequential graph\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.model(x)\n",
    "        out = self.output(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForwardPlus(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (7): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "  )\n",
      "  (output): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n",
      "Accuracy before training: 0.33333334\n",
      "Accuracy after training: 0.93333334\n"
     ]
    }
   ],
   "source": [
    "# fix the seed for reproducibility\n",
    "seed = 42\n",
    "fix_random(seed)\n",
    "\n",
    "\n",
    "# Start tensorboard\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "# Define the architecture, loss and optimizer\n",
    "depth = 2  #  new hyperparameter\n",
    "model = FeedForwardPlus(my_dataset.num_features, hidden_size, my_dataset.num_classes, depth)\n",
    "print(model)\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Test before the training\n",
    "y_test, y_pred_c, _ = test_model(model, test_loader, device)\n",
    "acc = (y_test == y_pred_c).float().sum() / y_test.shape[0]\n",
    "print(\"Accuracy before training:\", acc.cpu().numpy())\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model = train_model(model, criterion, optimizer, num_epochs, train_loader, val_loader, device, writer)\n",
    "\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"models/model\"))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Test after the training\n",
    "y_test, y_pred_c, _ = test_model(model, test_loader, device)\n",
    "acc = (y_test == y_pred_c).float().sum() / y_test.shape[0]\n",
    "print(\"Accuracy after training:\", acc.cpu().numpy())\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_test.cpu(), y_pred_c.cpu()))\n",
    "\n",
    "\n",
    "# Close tensorboard writer after a training\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "fix_random(seed)\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "hidden_sizes = [16, 32]\n",
    "depths = [2, 4]\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "import itertools\n",
    "\n",
    "hyperparameters = itertools.product(hidden_sizes, depths)\n",
    "\n",
    "\n",
    "# grid search loop\n",
    "for hidden_size, depth in hyperparameters:\n",
    "    fix_random(seed)\n",
    "\n",
    "    log_name = \"dim\" + str(hidden_size) + \"-dp\" + str(depth) + \"-ep\" + str(num_epochs) + \"-lr\" + str(learning_rate)\n",
    "\n",
    "    # start tensorboard\n",
    "    writer = SummaryWriter(\"runs/\" + log_name)\n",
    "\n",
    "    # define architecture, loss and optimizer\n",
    "    model = FeedForwardPlus(my_dataset.num_features, hidden_size, my_dataset.num_classes, depth)\n",
    "    model.to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # train\n",
    "    model = train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        num_epochs,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        device,\n",
    "        writer,\n",
    "        log_name,\n",
    "    )\n",
    "\n",
    "    writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the best model on the test set: 0.93333334\n"
     ]
    }
   ],
   "source": [
    "# Choose and load the best model and evaluate it on the test set\n",
    "\n",
    "# Re-instantiate the model and read best weights\n",
    "model = FeedForwardPlus(my_dataset.num_features, 16, my_dataset.num_classes, 4)\n",
    "model.load_state_dict(torch.load(\"models/dim16-dp4-ep1000-lr0.01\"))\n",
    "model.to(device)\n",
    "\n",
    "y_test, y_pred_c, _ = test_model(model, test_loader, device)\n",
    "acc = (y_test == y_pred_c).float().sum() / y_test.shape[0]\n",
    "\n",
    "print(\"Accuracy of the best model on the test set:\", acc.cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
