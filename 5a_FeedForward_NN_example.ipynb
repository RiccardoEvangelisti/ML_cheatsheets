{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\evang.HOMEEVANGELISTI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.tensorboard import SummaryWriter  # log writer to visualize the loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"NN_outputs/\"\n",
    "model_dir = base_dir + \"models/\"\n",
    "runs_dir = base_dir + \"runs/\"\n",
    "\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "shutil.rmtree(runs_dir, ignore_errors=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(runs_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility, fix all the seeds\n",
    "\n",
    "\n",
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Layer class\n",
    "# Extend the abstract class \"Dataset\"\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    # Save X and y as Tensors, accordingly to the type of the data\n",
    "    # https://pytorch.org/docs/stable/tensors.html\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "\n",
    "        # Useful attributes\n",
    "        self.num_features = X.shape[1]\n",
    "        self.num_classes = len(np.unique(y))\n",
    "\n",
    "    # Dataset size\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    # Fetch a data sample (single sample or batch) for a given index/es\n",
    "    # (if the dataset is not in memory, it can read from file system and return the object)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network class\n",
    "# Extend the abstract class \"Module\"\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        # Useful attributes\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Definition of Layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden_size)  # input to hidden\n",
    "        self.fc2 = nn.Linear(self.hidden_size, num_classes)  # hidden to output\n",
    "\n",
    "        # Activation Function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    # How layers are connected between them\n",
    "    # This even defines the graph of backpropagation\n",
    "    def forward(self, x):\n",
    "        h = self.fc1(x)  # first layer\n",
    "        h = self.relu(h)  # activation function\n",
    "        output = self.fc2(h)  # second layer\n",
    "        return output\n",
    "\n",
    "    def _get_name(self):\n",
    "        return \"FeedForward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the training process\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,  # instance of class to train\n",
    "    criterion,  # instance of loss function\n",
    "    optimizer,  # instance of optimizer\n",
    "    epochs,  # number of\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    device,  # to train on\n",
    "    log_writer,\n",
    "    log_name,\n",
    "):\n",
    "    n_iter = 0\n",
    "    best_valid_loss = float(\"inf\")  # initialized to worst possible value\n",
    "\n",
    "    # EPOCHS\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # activate training mode (for BatchNorm or Dropout)\n",
    "\n",
    "        # BATCHES\n",
    "        for data, targets in train_loader:  # get_item from MyDataset class (single item or batch)\n",
    "            data, targets = data.to(device), targets.to(device)  # move data and targets to cpu/gpu\n",
    "\n",
    "            optimizer.zero_grad()  # gradient to zero\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(data)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred, targets)\n",
    "            log_writer.add_scalar(\"Loss/train\", loss, n_iter)  # plot the batches\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_iter += 1\n",
    "\n",
    "        # Valuation\n",
    "        y_test, _, y_pred = test_model(model, val_loader, device)\n",
    "        loss_val = criterion(y_pred, y_test)\n",
    "        log_writer.add_scalar(\"Loss/val\", loss_val, epoch)  # plot the epochs\n",
    "\n",
    "        # Save the model with best loss through the epochs\n",
    "        if loss_val.item() < best_valid_loss:\n",
    "            best_valid_loss = loss_val.item()\n",
    "            torch.save(model.state_dict(), model_dir + log_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the performance on Validation and Test sets\n",
    "\n",
    "\n",
    "def test_model(model: nn.Module, data_loader: DataLoader, device) -> tuple[Tensor, Tensor, Tensor]:\n",
    "    \"\"\"return:\n",
    "    - y_test - true lables\n",
    "    - y_pred_c - has 1 column, where each element is the predicted lable with bigger probability among the \"c\" predicted\n",
    "    - y_pred - has \"c\" columns as the number of classes of the test set\n",
    "    \"\"\"\n",
    "    model.eval()  # activate evaluation mode (for BatchNorm or Dropout)\n",
    "\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "\n",
    "    for data, targets in data_loader:\n",
    "        data, targets = data.to(device), targets.to(device)  # move data and targets to cpu/gpu\n",
    "        y_pred.append(model(data))  # accumulate predictions\n",
    "        y_test.append(targets)  # accumulate labels\n",
    "\n",
    "    y_test = torch.stack(y_test).squeeze()  # it's one column (each row is a different sample)\n",
    "    y_pred = torch.stack(\n",
    "        y_pred\n",
    "    ).squeeze()  # there are \"c\" columns as the number of classes. Each column is the probability (as float number) to that class (each row is a different sample)\n",
    "    y_pred_c = y_pred.argmax(\n",
    "        dim=1, keepdim=True\n",
    "    ).squeeze()  # return max position of prediction array: that is the class I will associate with the sample\n",
    "    return y_test, y_pred_c, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# look for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "num_epochs = 500\n",
    "learning_rate = 0.01\n",
    "batch_size = 32  # the SIZE of one batch, not the total number of batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoaders preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train/val/test DataLoaders\n",
    "\n",
    "# Load\n",
    "data = datasets.load_iris()\n",
    "X = data[\"data\"]\n",
    "y = data[\"target\"]\n",
    "indices = np.arange(X.shape[0])  # useful later to split the data in train/val/test\n",
    "\n",
    "\n",
    "# Separate indices in train/val/set\n",
    "# \"stratify=y\" makes sure to keep the classes proportions on the dataset (useful on imbalanced classes)\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, stratify=y, random_state=seed)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.2, stratify=y[train_idx], random_state=seed)\n",
    "\n",
    "\n",
    "# Scale data\n",
    "train_mean = np.mean(X[train_idx, :], axis=0)\n",
    "train_std = np.std(X[train_idx, :], axis=0)  # use only train\n",
    "X = (X - train_mean) / train_std  # but apply to all dataset\n",
    "\n",
    "\n",
    "# DataLoaders\n",
    "my_dataset = MyDataset(X, y)\n",
    "\n",
    "train_subset = Subset(my_dataset, train_idx)\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_subset = Subset(my_dataset, val_idx)\n",
    "val_loader = DataLoader(val_subset, batch_size=1)\n",
    "\n",
    "test_subset = Subset(my_dataset, test_idx)\n",
    "test_loader = DataLoader(test_subset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model, Criterion, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random(seed)\n",
    "\n",
    "hidden_size = 32\n",
    "model = FeedForward(my_dataset.num_features, hidden_size, my_dataset.num_classes)\n",
    "model.to(device)  # move the NN to device\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "log_writer = SummaryWriter(runs_dir + model._get_name())  # Start tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run\n",
    "Run Tensorboard from the command line:\n",
    "\n",
    "> tensorboard --logdir nn/runs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before training: 0.46666667\n",
      "Accuracy after training: 0.93333334\n",
      "FeedForward(\n",
      "  (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=3, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Test before the training\n",
    "y_test, y_pred_c, _ = test_model(model, test_loader, device)\n",
    "acc = (y_test == y_pred_c).float().sum() / y_test.shape[0]\n",
    "print(\"Accuracy before training:\", acc.cpu().numpy())\n",
    "\n",
    "\n",
    "# Train\n",
    "train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    log_writer,\n",
    "    model._get_name(),\n",
    ")\n",
    "\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(model_dir + model._get_name()))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Test after the training\n",
    "y_test, y_pred_c, _ = test_model(model, test_loader, device)\n",
    "acc = (y_test == y_pred_c).float().sum() / y_test.shape[0]\n",
    "print(\"Accuracy after training:\", acc.cpu().numpy())\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "# Close tensorboard writer after a training\n",
    "log_writer.flush()\n",
    "log_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardDeep(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, depth=1):\n",
    "        super(FeedForwardDeep, self).__init__()\n",
    "\n",
    "        model = [\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "        ]\n",
    "\n",
    "        # Set of pytorch modules\n",
    "        block = [\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),  # BatchNorm 1 dimention\n",
    "            nn.ReLU(),\n",
    "        ]\n",
    "\n",
    "        for i in range(depth):\n",
    "            model += block\n",
    "\n",
    "        # Create sequential graph\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.model(x)\n",
    "        out = self.output(h)\n",
    "        return out\n",
    "\n",
    "    def _get_name(self):\n",
    "        return \"FeedForwardDeep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "fix_random(seed)\n",
    "\n",
    "\n",
    "model = FeedForwardDeep(my_dataset.num_features, hidden_size, my_dataset.num_classes, depth=2)\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Start tensorboard\n",
    "log_writer = SummaryWriter(runs_dir + model._get_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before training: 0.33333334\n",
      "Accuracy after training: 1.0\n",
      "FeedForwardDeep(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (7): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "  )\n",
      "  (output): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Test before the training\n",
    "y_test, y_pred_c, _ = test_model(model, test_loader, device)\n",
    "acc = (y_test == y_pred_c).float().sum() / y_test.shape[0]\n",
    "print(\"Accuracy before training:\", acc.cpu().numpy())\n",
    "\n",
    "\n",
    "# Train the model\n",
    "train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    log_writer,\n",
    "    model._get_name(),\n",
    ")\n",
    "\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(model_dir + model._get_name()))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Test after the training\n",
    "y_test, y_pred_c, _ = test_model(model, test_loader, device)\n",
    "acc = (y_test == y_pred_c).float().sum() / y_test.shape[0]\n",
    "print(\"Accuracy after training:\", acc.cpu().numpy())\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_test.cpu(), y_pred_c.cpu()))\n",
    "print(model)\n",
    "\n",
    "\n",
    "# Close tensorboard writer after a training\n",
    "log_writer.flush()\n",
    "log_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# GridSearch over hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "hidden_sizes = [16, 32]\n",
    "depths = [2, 4]\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "hyperparameters = itertools.product(hidden_sizes, depths)\n",
    "\n",
    "# grid search loop\n",
    "for hidden_size, depth in hyperparameters:\n",
    "    fix_random(seed)\n",
    "\n",
    "    # Define architecture, loss and optimizer\n",
    "    model = FeedForwardDeep(my_dataset.num_features, hidden_size, my_dataset.num_classes, depth)\n",
    "    model.to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    log_name = (\n",
    "        model._get_name()\n",
    "        + \"_\"\n",
    "        + \"dim\"\n",
    "        + str(hidden_size)\n",
    "        + \"-dp\"\n",
    "        + str(depth)\n",
    "        + \"-ep\"\n",
    "        + str(num_epochs)\n",
    "        + \"-lr\"\n",
    "        + str(learning_rate)\n",
    "    )\n",
    "\n",
    "    # Start tensorboard\n",
    "    log_writer = SummaryWriter(runs_dir + log_name)\n",
    "\n",
    "    # Train\n",
    "    train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        num_epochs,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        device,\n",
    "        log_writer,\n",
    "        log_name,\n",
    "    )\n",
    "\n",
    "    log_writer.flush()\n",
    "log_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the best model on the test set: 0.93333334\n"
     ]
    }
   ],
   "source": [
    "# Choose and load the best model and evaluate it on the test set\n",
    "\n",
    "# Re-instantiate the model and read best weights\n",
    "model = FeedForwardDeep(my_dataset.num_features, 16, my_dataset.num_classes, 4)\n",
    "model.load_state_dict(torch.load(model_dir + model._get_name() + \"_\" + \"dim16-dp4-ep1000-lr0.01\"))\n",
    "model.to(device)\n",
    "\n",
    "y_test, y_pred_c, _ = test_model(model, test_loader, device)\n",
    "acc = (y_test == y_pred_c).float().sum() / y_test.shape[0]\n",
    "\n",
    "print(\"Accuracy of the best model on the test set:\", acc.cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
