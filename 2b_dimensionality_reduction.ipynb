{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of dimensionality \n",
    "It refers to the problems associated with multivariate data analysis: for a given sample size, there is a maximum number of features above which the performance of our classifier will degrade rather than improve. In most cases, the extra information that is lost by discarding certain features is compensated by a more accurate mapping into the smaller dimensional space\n",
    "- Solutions: \n",
    "    - Add a priori knowledge to weigh more some variables instead of others; \n",
    "    - Reduce dimensionality by using unsupervised algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"datasets/winequality-red.csv\", sep=\";\")\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]  # last column\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA) (|||)\n",
    "- Unsupervised method: it does not consider the class labels of the samples (\"class separability\")\n",
    "- A new dataset is build, that will have only the attributes which capture most of the data variation. 1) Finds the eigenvector of the covariance matrix; 2) The eigenvectors define the new space.\n",
    "- Excluding the \"non-significant\" principal components may filter out the noise that is present in the data.\n",
    "- The maximum number of PCs that can be calculated is min(n_samples, n_features)\n",
    "- The number of PCs to keep is the one that preserves at least 95% of the total variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.48561286e-01 4.64263108e-02 2.51642763e-03 1.56686004e-03\n",
      " 8.54121349e-04 3.59942769e-05 1.97718297e-05 9.54972714e-06\n",
      " 8.45436759e-06 1.22342746e-06]\n",
      "Sum: 0.9999999995231198\n"
     ]
    }
   ],
   "source": [
    "# n_components is how many PCs to keep\n",
    "pca = PCA(random_state=42, n_components=\"mle\")\n",
    "pca.fit(X_train)\n",
    "\n",
    "X_train_t = pca.transform(X_train)\n",
    "X_test_t = pca.transform(X_test)\n",
    "\n",
    "# Percentage of variance explained by each of the selected components\n",
    "# The sum needs to be at least 95%, otherwise too much information is lost\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(\"Sum:\", pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94856129 0.9949876  0.99750402 0.99907088 0.99992501 0.999961\n",
      " 0.99998077 0.99999032 0.99999878 1.        ]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Sort the explained variance ratios in descending order\n",
    "ratios = np.flip(np.sort(pca.explained_variance_ratio_))\n",
    "\n",
    "# Calculate the cumulative sum of the explained variance ratios\n",
    "cum_sum = np.cumsum(ratios)\n",
    "\n",
    "print(cum_sum)\n",
    "\n",
    "# Find the index i of the first principal component where the cumulative sum exceeds 0.999\n",
    "i = np.argmax(cum_sum > 0.999)\n",
    "print(i)\n",
    "\n",
    "# Keep the first i principal components and discard the rest\n",
    "X_train_t = X_train_t[:, :i]\n",
    "X_test_t = X_test_t[:, :i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: original: 0.5625\n",
      "Classification Accuracy: transformed by PCA: 0.49583333333333335\n",
      "Classification Accuracy: transformed by PCA, selecting only most valuable variable: 0.4791666666666667\n"
     ]
    }
   ],
   "source": [
    "# Classification with original data\n",
    "clf = LogisticRegression(solver=\"liblinear\", random_state=42, max_iter=300)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Classification Accuracy: original:\", clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# Classification with PCA data transformed\n",
    "clf = LogisticRegression(solver=\"liblinear\", random_state=42, max_iter=300)\n",
    "clf.fit(X_train_t, y_train)\n",
    "print(\"Classification Accuracy: transformed by PCA:\", clf.score(X_test_t, y_test))\n",
    "\n",
    "# Classification with PCA data transformed and selecting ONLY the most valuable attribute (the first)\n",
    "col = 0\n",
    "clf = LogisticRegression(solver=\"liblinear\", random_state=42, max_iter=300)\n",
    "X_train_t_head = X_train_t[:, col].reshape(-1, 1)\n",
    "X_test_t_head = X_test_t[:, col].reshape(-1, 1)\n",
    "clf.fit(X_train_t_head, y_train)\n",
    "print(\n",
    "    \"Classification Accuracy: transformed by PCA, selecting only most valuable variable:\",\n",
    "    clf.score(X_test_t_head, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Linear Discriminant Analysis\n",
    "- Supervised method: considers class separability\n",
    "- It works projecting the input data to a linear subspace consisting of the directions which maximize the separation between classes\n",
    "- The maximum number of PCs that can be calculated is min(n_classes - 1, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(solver=\"eigen\")\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "X_train_t = lda.transform(X_train)\n",
    "X_test_t = lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: original: 0.5625\n",
      "Classification Accuracy: transformed by LDA: 0.5583333333333333\n",
      "Classification Accuracy: transformed by LDA, selecting only most valuable variable: 0.40625\n"
     ]
    }
   ],
   "source": [
    "# Classification with original data\n",
    "clf = LogisticRegression(solver=\"liblinear\", random_state=42, max_iter=200)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Classification Accuracy: original:\", clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# Classification with data transformed with LDA\n",
    "clf = LogisticRegression(solver=\"liblinear\", random_state=42, max_iter=200)\n",
    "clf.fit(X_train_t, y_train)\n",
    "print(\"Classification Accuracy: transformed by LDA:\", clf.score(X_test_t, y_test))\n",
    "\n",
    "# Classification with data transformed with LDA and selecting ONLY the most valuable attribute (the first)\n",
    "col = 0\n",
    "clf = LogisticRegression(solver=\"liblinear\", random_state=42, max_iter=200)\n",
    "X_train_t_head = X_train_t[:, col].reshape(-1, 1)\n",
    "X_test_t_head = X_test_t[:, col].reshape(-1, 1)\n",
    "clf.fit(X_train_t_head, y_train)\n",
    "print(\n",
    "    \"Classification Accuracy: transformed by LDA, selecting only most valuable variable:\",\n",
    "    clf.score(X_test_t_head, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "- It's a technique for dimensionality reduction that is particularly well-suited for the visualization of high-dimensional datasets\n",
    "- More info:\n",
    "• https://lvdmaaten.github.io/tsne/\n",
    "• https://distill.pub/2016/misread-tsne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### SelectKBest\n",
    "- A type of Univariate Feature Selection [(doc)](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)\n",
    "- Selects the best features based on univariate statistical tests.\n",
    "- These objects take as input a scoring function that returns univariate scores and p-values:\n",
    "    - For regression: r_regression, f_regression, mutual_info_regression\n",
    "    - For classification: chi2, f_classif, mutual_info_classif\n",
    "- The methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation. Note that the chi2 test should only be applied to non-negative features, such as frequencies.\n",
    "- If you use sparse data (i.e. data represented as sparse matrices), chi2, mutual_info_regression, mutual_info_classif will deal with the data without making it dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = SelectKBest(f_classif, k=2).fit_transform(X, y)\n",
    "X_new.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
