{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of dimensionality \n",
    "It refers to the problems associated with multivariate data analysis: for a given sample size, there is a maximum number of features above which the performance of our classifier will degrade rather than improve. In most cases, the extra information that is lost by discarding certain features is compensated by a more accurate mapping into the smaller dimensional space\n",
    "- Solutions: \n",
    "    - Add a priori knowledge to weigh more some variables instead of others; \n",
    "    - Reduce dimensionality by using unsupervised algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"datasets/iris.data.txt\")\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]  # last column\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA) (|||)\n",
    "- Unsupervised method: it does not consider the class labels of the samples (\"class separability\")\n",
    "- A new dataset is build, that will have only the attributes which capture most of the data variation. 1) Finds the eigenvector of the covariance matrix; 2) The eigenvectors define the new space.\n",
    "- Excluding the \"non-significant\" principal components may filter out the noise that is present in the data.\n",
    "- The maximum number of PCs that can be calculated is min(n_samples, n_features)\n",
    "- The number of PCs to keep is the one that preserves at least 95% of the total variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.91939858 0.05522612 0.02020546]\n",
      "Sum: 0.9948301607584098\n"
     ]
    }
   ],
   "source": [
    "# n_components is how many PCs to keep\n",
    "pca = PCA(random_state=42, n_components=\"mle\")\n",
    "pca.fit(X_train)\n",
    "\n",
    "X_train_t = pca.transform(X_train)\n",
    "X_test_t = pca.transform(X_test)\n",
    "\n",
    "# Percentage of variance explained by each of the selected components\n",
    "# The sum needs to be at least 95%, otherwise too much information is lost\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(\"Sum:\", pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: original: 0.9777777777777777\n",
      "Classification Accuracy: transformed by PCA: 0.9111111111111111\n",
      "Classification Accuracy: transformed by PCA, selecting only most valuable variable: 0.8444444444444444\n"
     ]
    }
   ],
   "source": [
    "# Classification with original data\n",
    "clf = LogisticRegression(solver=\"liblinear\", random_state=42, max_iter=100)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Classification Accuracy: original:\", clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# Classification with PCA data transformed\n",
    "clf = LogisticRegression(solver=\"liblinear\", random_state=42, max_iter=100)\n",
    "clf.fit(X_train_t, y_train)\n",
    "print(\"Classification Accuracy: transformed by PCA:\", clf.score(X_test_t, y_test))\n",
    "\n",
    "# Classification with PCA data transformed and selecting ONLY the most valuable attribute (the first)\n",
    "col = 0\n",
    "clf = LogisticRegression(solver=\"liblinear\", random_state=42, max_iter=100)\n",
    "X_train_t_head = X_train_t[:, col].reshape(-1, 1)\n",
    "X_test_t_head = X_test_t[:, col].reshape(-1, 1)\n",
    "clf.fit(X_train_t_head, y_train)\n",
    "print(\n",
    "    \"Classification Accuracy: transformed by PCA, selecting only most valuable variable:\",\n",
    "    clf.score(X_test_t_head, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Linear Discriminant Analysis\n",
    "- Supervised method: considers class separability\n",
    "- It works projecting the input data to a linear subspace consisting of the directions which maximize the separation between classes\n",
    "- The maximum number of PCs that can be calculated is min(n_classes - 1, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(solver=\"eigen\")\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "X_train_t = lda.transform(X_train)\n",
    "X_test_t = lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: original: 0.9777777777777777\n",
      "Classification Accuracy: transformed by LDA: 0.9777777777777777\n",
      "Classification Accuracy: transformed by LDA, selecting only most valuable variable: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "# Classification with original data\n",
    "clf = LogisticRegression(solver=\"liblinear\", random_state=42, max_iter=200)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Classification Accuracy: original:\", clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# Classification with data transformed with LDA\n",
    "clf = LogisticRegression(solver=\"liblinear\", random_state=42, max_iter=200)\n",
    "clf.fit(X_train_t, y_train)\n",
    "print(\"Classification Accuracy: transformed by LDA:\", clf.score(X_test_t, y_test))\n",
    "\n",
    "# Classification with data transformed with LDA and selecting ONLY the most valuable attribute (the first)\n",
    "col = 0\n",
    "clf = LogisticRegression(solver=\"liblinear\", random_state=42, max_iter=200)\n",
    "X_train_t_head = X_train_t[:, col].reshape(-1, 1)\n",
    "X_test_t_head = X_test_t[:, col].reshape(-1, 1)\n",
    "clf.fit(X_train_t_head, y_train)\n",
    "print(\n",
    "    \"Classification Accuracy: transformed by LDA, selecting only most valuable variable:\",\n",
    "    clf.score(X_test_t_head, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "- It's a technique for dimensionality reduction that is particularly well-suited for the visualization of high-dimensional datasets\n",
    "- More info:\n",
    "• https://lvdmaaten.github.io/tsne/\n",
    "• https://distill.pub/2016/misread-tsne"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
